{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Topic**: Embeddings & Semantic Search\n",
    "\n",
    "ðŸš« **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In this script we will explore **embeddings and semantic search**. These tools complement the lexical search tools that we covered last week. By the end of this session, you'll be able to:\n",
    "- Understand what embeddings are and how they encode meaning\n",
    "- Use both local (Hugging Face) and API-based (OpenAI) embedding models\n",
    "- Implement semantic search from scratch using cosine similarity\n",
    "- Discover why similarity does not equal relevance\n",
    "- Build hybrid search combining BM25 + embeddings\n",
    "- Compare search approaches using Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using modules in your code\n",
    "\n",
    "Starting today, we'll use a helpers module to organize reusable code. Instead of copying functions between notebooks, we will import them in our code like so:\n",
    "\n",
    "```python\n",
    "from helpers import load_wands_products, snowball_tokenize, score_bm25\n",
    "```\n",
    "\n",
    "This is similar to how you're using third-party libraries like pandas, numpy, matplotlib, etc. It is also how a good, professional codebase works: modularity helps us keep the code clean and easy to maintain.\n",
    "- If scripts use the same functions, we only need to fix a bug once, and then it is fixed everywhere\n",
    "- It allows us to have cleaner notebooks: we can focus on the lesson and spend our time rewriting boilerplate code.\n",
    "- It makes our code reusable: we can use the same functions work across lectures and homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E402\n",
    "\n",
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# helpers imports\n",
    "from helpers import (\n",
    "    # Data loading\n",
    "    load_wands_products, load_wands_queries, load_wands_labels,\n",
    "    # BM25 \n",
    "    build_index, score_bm25, search_bm25,\n",
    "    # Evaluation\n",
    "    evaluate_search,\n",
    "    # Embeddings\n",
    "    get_local_model, batch_embed_local,\n",
    "    # Similarity\n",
    "    batch_cosine_similarity,\n",
    "    # Utility\n",
    "    normalize_scores\n",
    ")\n",
    "\n",
    "# Load environment variables for API keys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. From Keywords to Meaning\n",
    "\n",
    "## 1.1 Recap: BM25 and Lexical Search\n",
    "\n",
    "Last week, we built a search engine using **BM25** - a lexical search algorithm that:\n",
    "- Matches documents based on **exact token matches**\n",
    "- Uses **TF-IDF** scoring with saturation and length normalization\n",
    "- Gives you **precise control** over what matches\n",
    "\n",
    "Let's reload our WANDS data and BM25 index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products: 42,994\n",
      "Queries: 480\n",
      "Labels: 233,448\n"
     ]
    }
   ],
   "source": [
    "# Load the WANDS dataset (same as Lecture 3 and Homework 3)\n",
    "products = load_wands_products()\n",
    "queries = load_wands_queries()\n",
    "labels = load_wands_labels()\n",
    "\n",
    "print(f\"Products: {len(products):,}\")\n",
    "print(f\"Queries: {len(queries):,}\")\n",
    "print(f\"Labels: {len(labels):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index contains 25,570 unique terms\n"
     ]
    }
   ],
   "source": [
    "# Build BM25 index on product names\n",
    "name_index, name_lengths = build_index(products['product_name'].tolist())\n",
    "print(f\"Index contains {len(name_index):,} unique terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Limitations of Lexical Search\n",
    "\n",
    "BM25 is powerful, but it has a fundamental limitation: it only matches **exact tokens**.\n",
    "\n",
    "What happens when we search for synonyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 results for 'couch':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>bm25_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10320</th>\n",
       "      <td>nava couch owl throw pillow</td>\n",
       "      <td>8.021734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>rundle couch bubbles throw pillow</td>\n",
       "      <td>8.021734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>double chaise lounge floor couch</td>\n",
       "      <td>8.021734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23758</th>\n",
       "      <td>double chaise lounge sofa floor couch</td>\n",
       "      <td>7.502148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>extra large and wide couch riser</td>\n",
       "      <td>7.502148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                product_name  bm25_score\n",
       "10320            nava couch owl throw pillow    8.021734\n",
       "10322      rundle couch bubbles throw pillow    8.021734\n",
       "824         double chaise lounge floor couch    8.021734\n",
       "23758  double chaise lounge sofa floor couch    7.502148\n",
       "1217        extra large and wide couch riser    7.502148"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for \"couch\"\n",
    "couch_results = search_bm25(\"couch\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'couch':\")\n",
    "couch_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 results for 'sofa':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>bm25_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21389</th>\n",
       "      <td>child sofa</td>\n",
       "      <td>5.361661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20392</th>\n",
       "      <td>kids sofa</td>\n",
       "      <td>5.361661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17784</th>\n",
       "      <td>sofa bed adjustable folding futon sofa video gaming sofa lounge sofa with tw...</td>\n",
       "      <td>5.195259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42536</th>\n",
       "      <td>essonne kids sofa</td>\n",
       "      <td>4.930615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>glasgo kids sofa</td>\n",
       "      <td>4.930615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          product_name  \\\n",
       "21389                                                                       child sofa   \n",
       "20392                                                                        kids sofa   \n",
       "17784  sofa bed adjustable folding futon sofa video gaming sofa lounge sofa with tw...   \n",
       "42536                                                                essonne kids sofa   \n",
       "38496                                                                 glasgo kids sofa   \n",
       "\n",
       "       bm25_score  \n",
       "21389    5.361661  \n",
       "20392    5.361661  \n",
       "17784    5.195259  \n",
       "42536    4.930615  \n",
       "38496    4.930615  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for \"sofa\" - a synonym!\n",
    "sofa_results = search_bm25(\"sofa\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'sofa':\")\n",
    "sofa_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 results for 'place to sit and relax':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>bm25_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17962</th>\n",
       "      <td>sitting and praying buddha statue</td>\n",
       "      <td>11.543951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29147</th>\n",
       "      <td>michaud eiffel tower peel and place wall decal</td>\n",
       "      <td>10.786498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27964</th>\n",
       "      <td>girl sitting on books and reading statue</td>\n",
       "      <td>10.139467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27959</th>\n",
       "      <td>girl sitting and reading a book statue</td>\n",
       "      <td>10.139467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27801</th>\n",
       "      <td>hibbing girl sitting and reading a book statue</td>\n",
       "      <td>9.558033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         product_name  bm25_score\n",
       "17962               sitting and praying buddha statue   11.543951\n",
       "29147  michaud eiffel tower peel and place wall decal   10.786498\n",
       "27964        girl sitting on books and reading statue   10.139467\n",
       "27959          girl sitting and reading a book statue   10.139467\n",
       "27801  hibbing girl sitting and reading a book statue    9.558033"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for something that is not a product name but should match both sofas and couches\n",
    "relax_results = search_bm25(\"place to sit and relax\", name_index, products, name_lengths, k=5)\n",
    "print(\"BM25 results for 'place to sit and relax':\")\n",
    "relax_results[['product_name', 'bm25_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the problem:**\n",
    "- \"couch\" results contain products with \"couch\" in the name, \"sofa\" results contain products with \"sofa\" in the name, but they are **synonyms** - a user searching for \"couch\" would probably want sofas too!\n",
    "- \"Place to sit and relax\" is a conceptual query that should match both sofas and couches, and the results we get back are not relevant at all.\n",
    "\n",
    "BM25 treats them as completely different words because it only matches exact tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. What are embeddings?\n",
    "\n",
    "> **TERM: Embedding**  \n",
    "> A **dense vector representation** that maps text (or other data) to a point in high-dimensional space where **semantically similar items are close together**.\n",
    "\n",
    "Think of it as assigning \"coordinates\" to the **meaning** of text:\n",
    "- \"couch\" and \"sofa\" would have similar coordinates (close together)\n",
    "- \"couch\" and \"refrigerator\" would have different coordinates (far apart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Getting Your First Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:1259\u001b[39m, in \u001b[36mOpenAIChatCompletion.embedding\u001b[39m\u001b[34m(self, model, input, timeout, logging_obj, model_response, optional_params, api_key, api_base, client, aembedding, max_retries, shared_session)\u001b[39m\n\u001b[32m   1258\u001b[39m headers: Optional[Dict] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m headers, sync_embedding_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_embedding_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1266\u001b[39m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:237\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:1132\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_embedding_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:1124\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_embedding_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1128\u001b[39m     headers = \u001b[38;5;28mdict\u001b[39m(raw_response.headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/main.py:4713\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(model, input, dimensions, encoding_format, timeout, api_base, api_version, api_key, api_type, caching, user, custom_llm_provider, litellm_call_id, logger_fn, **kwargs)\u001b[39m\n\u001b[32m   4712\u001b[39m     \u001b[38;5;66;03m## EMBEDDING CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4713\u001b[39m     response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4715\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbeddingResponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4721\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4722\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4723\u001b[39m \u001b[43m        \u001b[49m\u001b[43maembedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43maembedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4726\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4727\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mdatabricks\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py:1290\u001b[39m, in \u001b[36mOpenAIChatCompletion.embedding\u001b[39m\u001b[34m(self, model, input, timeout, logging_obj, model_response, optional_params, api_key, api_base, client, aembedding, max_retries, shared_session)\u001b[39m\n\u001b[32m   1289\u001b[39m     error_headers = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m   1291\u001b[39m     status_code=status_code, message=error_text, headers=error_headers\n\u001b[32m   1292\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlitellm\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Get an embedding using OpenAI's API via LiteLLM\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This is what happens inside get_embedding_openai()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcouch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m couch_emb = np.array(response.data[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mType: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(couch_emb)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/utils.py:1739\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1736\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1737\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1738\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/utils.py:1560\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1558\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1559\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1563\u001b[39m     kwargs=kwargs,\n\u001b[32m   1564\u001b[39m     call_type=call_type,\n\u001b[32m   1565\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/main.py:5472\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(model, input, dimensions, encoding_format, timeout, api_base, api_version, api_key, api_type, caching, user, custom_llm_provider, litellm_call_id, logger_fn, **kwargs)\u001b[39m\n\u001b[32m   5466\u001b[39m litellm_logging_obj.post_call(\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5468\u001b[39m     api_key=api_key,\n\u001b[32m   5469\u001b[39m     original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   5470\u001b[39m )\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5472\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5474\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5477\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2356\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2355\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2358\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ai-engineering-fordham/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:375\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ExceptionCheckers.is_error_str_rate_limit(error_str):\n\u001b[32m    374\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    376\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m         model=model,\n\u001b[32m    378\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    379\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    380\u001b[39m     )\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ExceptionCheckers.is_error_str_context_window_exceeded(error_str):\n\u001b[32m    382\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: OpenAIException - Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "\n",
    "# Get an embedding using OpenAI's API via LiteLLM\n",
    "# This is what happens inside get_embedding_openai()\n",
    "response = litellm.embedding(model=\"text-embedding-3-small\", input=[\"couch\"])\n",
    "couch_emb = np.array(response.data[0][\"embedding\"])\n",
    "\n",
    "print(f\"Type: {type(couch_emb)}\")\n",
    "print(f\"Dimension: {len(couch_emb)}\")\n",
    "print(f\"First 10 values: {couch_emb[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is a **1536-dimensional vector** of floating-point numbers.\n",
    "\n",
    "Each dimension captures some aspect of the word's meaning - but unlike features we design ourselves, these are **latent features** learned by the model.\n",
    "\n",
    "> **TERM: Latent Features**  \n",
    "> Hidden dimensions in the embedding that capture abstract concepts. They're not directly interpretable like \"color=red\" or \"size=large\" - they're patterns the model discovered during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Measuring Similarity with Cosine\n",
    "\n",
    "To find similar items, we measure the \"distance\" between embeddings using **cosine similarity**:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$\n",
    "\n",
    "- **1.0** = identical direction (most similar)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (most dissimilar)\n",
    "\n",
    "Why cosine? It focuses on **direction** (meaning) not **magnitude** (length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Embeddings Capture Meaning\n",
    "\n",
    "Let's see how embeddings capture the relationship between words. We'll compute cosine similarity for each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for related words\n",
    "words = [\"couch\", \"sofa\", \"chair\", \"table\", \"refrigerator\"]\n",
    "\n",
    "# Get embeddings using OpenAI API via LiteLLM\n",
    "embeddings = {}\n",
    "for word in words:\n",
    "    response = litellm.embedding(model=\"text-embedding-3-large\", input=[word])\n",
    "    embeddings[word] = np.array(response.data[0][\"embedding\"])\n",
    "\n",
    "# Calculate similarity between all pairs using cosine similarity\n",
    "print(\"Similarity matrix:\")\n",
    "\n",
    "word_width = max(len(w) for w in words) + 2\n",
    "header = \"\".join([f\"{'':<{word_width}s}\"] + [f\"{w:>{word_width}s}\" for w in words])\n",
    "print(header)\n",
    "print(\"-\" * (word_width + len(words) * word_width))\n",
    "\n",
    "for w1 in words:\n",
    "    row = [f\"{w1:<{word_width}s}\"]\n",
    "    for w2 in words:\n",
    "        # Cosine similarity: dot product divided by product of norms\n",
    "        sim = np.dot(embeddings[w1], embeddings[w2]) / (np.linalg.norm(embeddings[w1]) * np.linalg.norm(embeddings[w2]))\n",
    "        row.append(f\"{sim:>{word_width}.2f}\")\n",
    "    print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice?**\n",
    "- \"couch\" and \"sofa\" have **very high similarity** (~0.75) - the model knows they're semantically similar\n",
    "- Furniture items (couch, sofa, chair, table) are more similar to each other\n",
    "- \"refrigerator\" is less similar to the furniture items\n",
    "\n",
    "The embedding model learned these relationships from training on massive amounts of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for related words\n",
    "words_2 = [\"Apostolos Filippas\", \"Tilda Swinton\", \"Technology\", \"Movies\", \"Suspiria\", \"Greek\", \"British\"]\n",
    "\n",
    "# Get embeddings using OpenAI API via LiteLLM\n",
    "embeddings_2 = {}\n",
    "for word in words_2:\n",
    "    response = litellm.embedding(model=\"text-embedding-3-large\", input=[word])\n",
    "    embeddings_2[word] = np.array(response.data[0][\"embedding\"])\n",
    "\n",
    "# Calculate similarity between all pairs\n",
    "print(\"Similarity matrix:\")\n",
    "\n",
    "word_width = max(len(w) for w in words_2) + 2\n",
    "header = \"\".join([f\"{'':<{word_width}s}\"] + [f\"{w:>{word_width}s}\" for w in words_2])\n",
    "print(header)\n",
    "print(\"-\" * (word_width + len(words_2) * word_width))\n",
    "\n",
    "for w1 in words_2:\n",
    "    row = [f\"{w1:<{word_width}s}\"]\n",
    "    for w2 in words_2:\n",
    "        # Inline cosine similarity\n",
    "        sim = np.dot(embeddings_2[w1], embeddings_2[w2]) / (np.linalg.norm(embeddings_2[w1]) * np.linalg.norm(embeddings_2[w2]))\n",
    "        row.append(f\"{sim:>{word_width}.2f}\")\n",
    "    print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.5 Local vs API Embeddings\n",
    "\n",
    "> **TERM: Hugging Face**  \n",
    "> An open-source platform hosting thousands of pre-trained AI models. Think of it as \"GitHub for AI models\" - you can download and run models locally without API calls or costs.\n",
    "\n",
    "So far we've used OpenAI's embedding API. But there's another option: **run models locally**!\n",
    "\n",
    "## 2.5.1 Loading a Local Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a local embedding - first call downloads the model (~80MB)\n",
    "model = get_local_model(\"all-MiniLM-L6-v2\")\n",
    "local_emb = model.encode(\"wooden coffee table\", convert_to_numpy=True)\n",
    "\n",
    "print(f\"Local embedding dimension: {len(local_emb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dimensions\n",
    "api_response = litellm.embedding(model=\"text-embedding-3-small\", input=[\"wooden coffee table\"])\n",
    "api_emb = np.array(api_response.data[0][\"embedding\"])\n",
    "\n",
    "print(f\"OpenAI (API): {len(api_emb)} dimensions\")\n",
    "print(f\"MiniLM (Local): {len(local_emb)} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5.3 Trade-offs: API vs Local\n",
    "\n",
    "| Aspect | API (OpenAI) | Local (Hugging Face) |\n",
    "|--------|-------------|---------------------|\n",
    "| **Cost** | ~$0.02 per 1M tokens | FREE |\n",
    "| **Dimensions** | 1536 (more expressive) | 384 (more compact) |\n",
    "| **Quality** | Generally higher | Good for most tasks |\n",
    "| **Speed** | Network latency | Faster for batches |\n",
    "| **Privacy** | Data sent to API | Data stays local |\n",
    "| **Setup** | Just API key | Downloads model (~80MB) |\n",
    "\n",
    "**When to use which?**\n",
    "- **Prototyping/Learning**: Local - free experimentation!\n",
    "- **Production with privacy needs**: Local\n",
    "- **Production needing best quality**: API\n",
    "- **High volume, cost-sensitive**: Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Measuring Similarity with Cosine\n",
    "\n",
    "## 3.1 Why Cosine Similarity?\n",
    "\n",
    "To find similar items, we need to measure the \"distance\" between embeddings. **Cosine similarity** measures the angle between two vectors:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(a, b) = \\frac{a \\cdot b}{\\|a\\| \\times \\|b\\|}$$\n",
    "\n",
    "- **1.0** = identical direction (most similar)\n",
    "- **0.0** = perpendicular (unrelated)\n",
    "- **-1.0** = opposite direction (most dissimilar)\n",
    "\n",
    "Why cosine instead of Euclidean distance? Cosine focuses on **direction** (meaning) not **magnitude** (length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine similarity formula implemented manually:\n",
    "def cosine_similarity_manual(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# Verify it works\n",
    "sim = cosine_similarity_manual(embeddings[\"couch\"], embeddings[\"sofa\"])\n",
    "print(f\"Cosine similarity (couch, sofa): {sim:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Batch Similarity for Efficiency\n",
    "\n",
    "When searching, we need to compare one query against **thousands of products**. Doing this one-by-one is slow. Instead, we use **matrix operations**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all word embeddings into a matrix\n",
    "word_matrix = np.array([embeddings[w] for w in words])\n",
    "print(f\"Matrix shape: {word_matrix.shape}\")\n",
    "\n",
    "# Query embedding\n",
    "query_emb = embeddings[\"couch\"]\n",
    "\n",
    "# Calculate similarity to all words at once\n",
    "similarities = batch_cosine_similarity(query_emb, word_matrix)\n",
    "\n",
    "for word, sim in zip(words, similarities):\n",
    "    print(f\"{word:15s}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Building Semantic Search from Scratch\n",
    "\n",
    "Now let's build a working semantic search engine!\n",
    "\n",
    "## 4.1 The Semantic Search Pipeline\n",
    "\n",
    "1. **Embed all products** (offline, once)\n",
    "2. **Embed the query** (at search time)\n",
    "3. **Calculate similarity** between query and all products\n",
    "4. **Return top-k** most similar products\n",
    "\n",
    "## 4.2 Embedding Products\n",
    "\n",
    "For speed in class, we'll work with a sample of 10,000 products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get consistent sample (same for everyone)\n",
    "products_sample = products.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Working with {len(products_sample):,} products\")\n",
    "products_sample[['product_id', 'product_name', 'product_class']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text for embedding: combine name and class\n",
    "products_sample['embed_text'] = (\n",
    "    products_sample['product_name'].fillna('') + ' ' +\n",
    "    products_sample['product_class'].fillna('')\n",
    ")\n",
    "\n",
    "products_sample['embed_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all products using local model\n",
    "# This took me 3.5 seconds\n",
    "print(\"Embedding products...\")\n",
    "start = time.time()\n",
    "product_embeddings = batch_embed_local(\n",
    "    products_sample['embed_text'].tolist(),\n",
    "    show_progress=True\n",
    ")\n",
    "print(f\"Done in {time.time() - start:.1f}s\")\n",
    "print(f\"Embeddings shape: {product_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings so we don't have to recompute\n",
    "np.save('temp/product_embeddings_sample.npy', product_embeddings)\n",
    "products_sample.to_csv('temp/products_sample.csv', index=False)\n",
    "print(\"Saved embeddings and sample to 'scripts/temp/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Implementing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_local(query, product_embeddings, products_df, k=10):\n",
    "    \"\"\"Search products using local embedding similarity.\"\"\"\n",
    "    # 1. Embed the query\n",
    "    model = get_local_model(\"all-MiniLM-L6-v2\")\n",
    "    query_emb = model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    # 2. Calculate similarity to all products\n",
    "    similarities = batch_cosine_similarity(query_emb, product_embeddings)\n",
    "    \n",
    "    # 3. Get top-k indices\n",
    "    top_k_idx = np.argsort(-similarities)[:k]\n",
    "    \n",
    "    # 4. Build results DataFrame\n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['similarity'] = similarities[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search!\n",
    "results = semantic_search_local(\"couch\", product_embeddings, products_sample)\n",
    "results[['rank', 'product_name', 'product_class', 'similarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the synonym problem that BM25 couldn't solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index for the sample\n",
    "sample_index, sample_lengths = build_index(products_sample['product_name'].tolist())\n",
    "\n",
    "# Search for \"sofa\" with BM25\n",
    "bm25_results = search_bm25(\"sofa\", sample_index, products_sample, sample_lengths, k=10)\n",
    "print(\"BM25 for 'sofa':\")\n",
    "print(bm25_results[['product_name', 'bm25_score']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Search for \"sofa\" with semantic search\n",
    "sem_results = semantic_search_local(\"sofa\", product_embeddings, products_sample, k=10)\n",
    "print(\"Semantic for 'sofa':\")\n",
    "print(sem_results[['product_name', 'similarity']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic search finds both \"sofa\" AND \"couch\" products!** It understands they're related concepts.\n",
    "\n",
    "Let's try another query that BM25 struggles with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A conceptual query - no exact keyword match\n",
    "query = \"place to sit and relax\"\n",
    "\n",
    "bm25_results = search_bm25(query, sample_index, products_sample, sample_lengths, k=5)\n",
    "print(f\"BM25 for '{query}':\")\n",
    "print(bm25_results[['product_name', 'bm25_score']].to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "sem_results = semantic_search_local(query, product_embeddings, products_sample, k=5)\n",
    "print(f\"Semantic for '{query}':\")\n",
    "print(sem_results[['product_name', 'similarity']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Comparing lexical and semantic search\n",
    "\n",
    "Semantic search seems magical - it finds synonyms and understands concepts! But it doesn't always work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's quantify how well each search method performs using **Recall@k**:\n",
    "\n",
    "> **Recall@k** = What fraction of relevant items did we find in the top k results?\n",
    ">\n",
    "> Example: If there are 20 relevant products and we found 3 of them in our top 10, Recall@10 = 3/20 = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter queries to those with products in our sample\n",
    "sample_product_ids = set(products_sample['product_id'])\n",
    "sample_labels = labels[labels['product_id'].isin(sample_product_ids)]\n",
    "sample_query_ids = set(sample_labels['query_id'])\n",
    "sample_queries = queries[queries['query_id'].isin(sample_query_ids)]\n",
    "\n",
    "print(f\"Queries with products in sample: {len(sample_queries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BM25 on sample\n",
    "print(\"Evaluating BM25...\")\n",
    "bm25_eval = evaluate_search(\n",
    "    lambda q: search_bm25(q, sample_index, products_sample, sample_lengths, k=10),\n",
    "    sample_queries, sample_labels, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Semantic Search on sample\n",
    "print(\"Evaluating Semantic Search...\")\n",
    "semantic_eval = evaluate_search(\n",
    "    lambda q: semantic_search_local(q, product_embeddings, products_sample, k=10),\n",
    "    sample_queries, sample_labels, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare!\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*40)\n",
    "print(f\"BM25 Mean Recall@10:     {bm25_eval['recall'].mean():.4f}\")\n",
    "print(f\"Semantic Mean Recall@10: {semantic_eval['recall'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see when each method wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine evaluations\n",
    "comparison = bm25_eval.merge(semantic_eval, on=['query_id', 'query'], suffixes=('_bm25', '_semantic'))\n",
    "comparison['diff'] = comparison['recall_semantic'] - comparison['recall_bm25']\n",
    "\n",
    "print(f\"Semantic wins: {(comparison['diff'] > 0).sum()} queries\")\n",
    "print(f\"BM25 wins: {(comparison['diff'] < 0).sum()} queries\")\n",
    "print(f\"Tie: {(comparison['diff'] == 0).sum()} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries where semantic search wins big\n",
    "print(\"Queries where SEMANTIC wins:\")\n",
    "semantic_wins = comparison.nlargest(5, 'diff')\n",
    "semantic_wins[['query', 'recall_bm25', 'recall_semantic', 'diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries where BM25 wins big\n",
    "print(\"Queries where BM25 wins:\")\n",
    "bm25_wins = comparison.nsmallest(5, 'diff')\n",
    "bm25_wins[['query', 'recall_bm25', 'recall_semantic', 'diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Key Takeaway\n",
    "\n",
    "**Similarity is NOT the same as relevance!**\n",
    "\n",
    "The embedding model learned general semantic similarity, but:\n",
    "- It wasn't trained on e-commerce product search\n",
    "- It doesn't know whether product type is often more important than theme\n",
    "- It doesn't understand your specific business rules\n",
    "\n",
    "**Never assume embeddings will solve your search problem. Always evaluate with real relevance labels!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Hybrid Search: Best of Both Worlds\n",
    "\n",
    "Since BM25 and semantic search have different strengths, what if we **combine them**?\n",
    "\n",
    "## 6.1 Weighted Combination\n",
    "\n",
    "The simplest hybrid approach:\n",
    "1. Get BM25 scores (normalize to 0-1)\n",
    "2. Get semantic similarity scores (already 0-1)\n",
    "3. Combine: `hybrid = alpha * semantic + (1-alpha) * bm25`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, sample_index, product_embeddings, products_df, \n",
    "                  sample_lengths, alpha=0.5, k=10):\n",
    "    \"\"\"\n",
    "    Combine BM25 and semantic search.\n",
    "    \n",
    "    alpha: weight for semantic (1-alpha for BM25)\n",
    "    \"\"\"\n",
    "    # Get BM25 scores\n",
    "    bm25_scores = score_bm25(query, sample_index, len(products_df), sample_lengths)\n",
    "    bm25_norm = normalize_scores(bm25_scores)\n",
    "    \n",
    "    # Get semantic scores\n",
    "    model = get_local_model(\"all-MiniLM-L6-v2\")\n",
    "    query_emb = model.encode(query, convert_to_numpy=True)\n",
    "    semantic_scores = batch_cosine_similarity(query_emb, product_embeddings)\n",
    "    # Semantic scores are already roughly 0-1, but let's normalize too\n",
    "    semantic_norm = normalize_scores(semantic_scores)\n",
    "    \n",
    "    # Combine\n",
    "    combined_scores = alpha * semantic_norm + (1 - alpha) * bm25_norm\n",
    "    \n",
    "    # Get top-k\n",
    "    top_k_idx = np.argsort(-combined_scores)[:k]\n",
    "    \n",
    "    results = products_df.iloc[top_k_idx].copy()\n",
    "    results['hybrid_score'] = combined_scores[top_k_idx]\n",
    "    results['bm25_score'] = bm25_norm[top_k_idx]\n",
    "    results['semantic_score'] = semantic_norm[top_k_idx]\n",
    "    results['rank'] = range(1, k + 1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search\n",
    "query = \"star wars rug\"\n",
    "hybrid_results = hybrid_search(query, sample_index, product_embeddings, \n",
    "                               products_sample, sample_lengths, alpha=0.5)\n",
    "\n",
    "print(f\"Hybrid search for '{query}':\")\n",
    "hybrid_results[['rank', 'product_name', 'product_class', 'bm25_score', 'semantic_score', 'hybrid_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Finding the Optimal Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different alpha values\n",
    "alphas = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(f\"Evaluating alpha={alpha}...\")\n",
    "    eval_df = evaluate_search(\n",
    "        lambda q, a=alpha: hybrid_search(q, sample_index, product_embeddings, \n",
    "                               products_sample, sample_lengths, alpha=a),\n",
    "        sample_queries, sample_labels, k=10, verbose=False\n",
    "    )\n",
    "    results.append({\n",
    "        'alpha': alpha,\n",
    "        'mean_recall': eval_df['recall'].mean()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results_df['alpha'], results_df['mean_recall'], 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Alpha (0=BM25 only, 1=Semantic only)', fontsize=12)\n",
    "plt.ylabel('Mean Recall@10', fontsize=12)\n",
    "plt.title('Hybrid Search Performance vs Alpha', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_alpha = results_df.loc[results_df['mean_recall'].idxmax(), 'alpha']\n",
    "print(f\"\\nBest alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate hybrid with best alpha\n",
    "print(f\"Evaluating Hybrid (alpha={best_alpha})...\")\n",
    "hybrid_eval = evaluate_search(\n",
    "    lambda q: hybrid_search(q, sample_index, product_embeddings, \n",
    "                           products_sample, sample_lengths, alpha=best_alpha),\n",
    "    sample_queries, sample_labels, k=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"BM25 only:              {bm25_eval['recall'].mean():.4f}\")\n",
    "print(f\"Semantic only:          {semantic_eval['recall'].mean():.4f}\")\n",
    "print(f\"Hybrid (alpha={best_alpha}):     {hybrid_eval['recall'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid search often outperforms both individual methods!**\n",
    "\n",
    "This is because:\n",
    "- BM25 ensures exact keyword matches are found\n",
    "- Semantic adds synonym and concept matching\n",
    "- Together they cover each other's weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "| Concept | What It Is | Key Insight |\n",
    "|---------|-----------|-------------|\n",
    "| **Embedding** | Dense vector representing meaning | Similar items = close vectors |\n",
    "| **Local vs API** | Hugging Face vs OpenAI | Trade-off: cost vs quality |\n",
    "| **Cosine Similarity** | Measures angle between vectors | Range -1 to 1, direction matters |\n",
    "| **Semantic Search** | Find by meaning, not keywords | Handles synonyms, paraphrases |\n",
    "| **Similarity != Relevance** | Training data != your domain | Always evaluate with real labels! |\n",
    "| **Hybrid Search** | BM25 + Semantic combined | Often beats either alone |\n",
    "\n",
    "## Can You Do These?\n",
    "\n",
    "- [ ] Get embeddings using both OpenAI API and local Hugging Face models\n",
    "- [ ] Calculate cosine similarity between vectors\n",
    "- [ ] Implement semantic search from scratch\n",
    "- [ ] Explain why similarity is not the same as relevance\n",
    "- [ ] Build hybrid search combining BM25 + embeddings\n",
    "- [ ] Evaluate search quality using Recall\n",
    "- [ ] Choose between local and API embeddings based on requirements\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "| Problem | Solution |\n",
    "|---------|----------|\n",
    "| Semantic search returns wrong product types | Consider hybrid search or filtering |\n",
    "| Embeddings are slow | Use local model for development, batch operations |\n",
    "| Recall is low for semantic | Domain mismatch - consider fine-tuning |\n",
    "| Model download fails | Check internet connection, disk space |\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Hugging Face Model Hub](https://huggingface.co/models)\n",
    "- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - Compare embedding models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
