{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### \ud83c\udf93 **Professor**: Apostolos Filippas\n",
    "\n",
    "### \ud83d\udcd8 **Class**: AI Engineering\n",
    "\n",
    "### \ud83d\udccb **Topic**: You Can Just Build Things\n",
    "\n",
    "\ud83d\udeab **Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In our firstfour lectures, we've covered how\n",
    "1. We can call LLMs via APIs and get structured responses\n",
    "2. We can build lexical search with BM25\n",
    "3. We can build semantic search with embeddings\n",
    "4. We can combine lexical and semantic search into hybrid search\n",
    "\n",
    "Today you will put it all together by building a Retrieval Augmented Generation (RAG) system.\n",
    "- This is a question-answering bot that can answer questions about Fordham University\n",
    "- You will use real data scraped from the Fordham website.\n",
    "\n",
    "\n",
    "Your RAG pipeline will look like this:\n",
    "\n",
    "```\n",
    "User Question\n",
    "     \u2193\n",
    "1. RETRIEVE: Find relevant documents (search!)\n",
    "     \u2193\n",
    "2. AUGMENT: Stuff those documents into a prompt\n",
    "     \u2193\n",
    "3. GENERATE: Ask an LLM to answer using the context\n",
    "     \u2193\n",
    "Answer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Look at your data\n",
    "\n",
    "In `data/fordham-website.zip` you'll find **~9,500 Markdown files** scraped from Fordham's website. Each file is one page \u2014 admissions info, program descriptions, faculty pages, financial aid, campus life, and more.\n",
    "\n",
    "Your task: **look at the data**\n",
    "- The first step in any AI engineering or data science project should always be to familiarize yourself with the data.\n",
    "- I cannot stress this enough.. without this step, it's hard to build anything useful.\n",
    "\n",
    "Tips:\n",
    "- Unzip the archive and look at some of the files. \n",
    "- Open a few in a text editor. \n",
    "- Get a feel for what you're working with.\n",
    "- The first line of every file is always the **URL** of the page it was scraped from. The rest is the page content converted to Markdown. Here's an example \u2014 `gabelli-school-of-business_veterans.md`:\n",
    "\n",
    "```markdown\n",
    "https://www.fordham.edu/gabelli-school-of-business/veterans\n",
    "\n",
    "# Military Veterans & Active Duty Members of the Military\n",
    "\n",
    "## Transform Your Knowledge & Skills Into a Business Career for the Future\n",
    "\n",
    "As a veteran or an active duty member of the United States Armed Services,\n",
    "you have gained or are currently acquiring the invaluable organizational,\n",
    "leadership, analytics, and technical knowledge and skills that hiring\n",
    "managers seek. These transferrable skills provide a major advantage in\n",
    "emerging, business-related industries where innovation, a global mind-set,\n",
    "and the ability to lead individuals and teams in the continuously evolving\n",
    "work environment, are critical for success.\n",
    "\n",
    "By completing a graduate or undergraduate business degree at the Gabelli\n",
    "School of Business, you can prepare for a lifelong career in some of\n",
    "today's fastest-growing fields. ...\n",
    "\n",
    "### Study at a Top-Ranked, Military-Friendly University\n",
    "\n",
    "The Gabelli School of Business is part of Fordham University, the only\n",
    "New York City university to be among those ranked \"Best for Vets\" by\n",
    "Military Times. ...\n",
    "\n",
    "### Learn How the Yellow Ribbon Program Works\n",
    "\n",
    "The Yellow Ribbon GI Education Enhancement Program, or the Yellow Ribbon\n",
    "Program, is a part of the Post-9/11 Veterans Educational Assistance Act\n",
    "of 2008. ...\n",
    "```\n",
    "\n",
    "The filenames mirror the URL structure \u2014 underscores replace path separators (e.g. `gabelli-school-of-business_veterans.md` came from `/gabelli-school-of-business/veterans`). Some files are short (a few lines), others are quite long.\n",
    "\n",
    "- Once you've looked around, load the files into Python. Python's built-in `zipfile` module can read zip archives without extracting to disk. Load them into a list of dictionaries or a DataFrame with at least two fields: the filename (or a clean page name) and the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "def load_fordham_data(source_path):\n",
    "    data = []\n",
    "    \n",
    "    # Check if it's a zip file\n",
    "    if source_path.endswith('.zip') and os.path.exists(source_path):\n",
    "        print(f\"Loading from zip: {source_path}\")\n",
    "        with zipfile.ZipFile(source_path, 'r') as z:\n",
    "            file_list = [f for f in z.namelist() if f.endswith('.md')]\n",
    "            for file_name in file_list:\n",
    "                with z.open(file_name) as f:\n",
    "                    try:\n",
    "                        content = f.read().decode('utf-8')\n",
    "                    except UnicodeDecodeError:\n",
    "                        continue\n",
    "                    \n",
    "                    lines = content.split('\\n', 1)\n",
    "                    url = lines[0].strip() if lines else \"\"\n",
    "                    body = lines[1].strip() if len(lines) > 1 else \"\"\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"filename\": file_name,\n",
    "                        \"url\": url,\n",
    "                        \"content\": body\n",
    "                    })\n",
    "                    \n",
    "    # Check if it's a directory\n",
    "    elif os.path.exists(source_path) and os.path.isdir(source_path):\n",
    "        print(f\"Loading from directory: {source_path}\")\n",
    "        path = pathlib.Path(source_path)\n",
    "        files = list(path.glob('*.md'))\n",
    "        \n",
    "        for file_path in files:\n",
    "             try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if not lines: continue\n",
    "                    \n",
    "                    url = lines[0].strip()\n",
    "                    content = \"\".join(lines[1:]).strip()\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"filename\": file_path.name,\n",
    "                        \"url\": url,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "             except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"Source not found or invalid: {source_path}\")\n",
    "        # Try default locations if provided path fails?\n",
    "        # Let's inspect data/fordham-website just in case user passed zip but has dir\n",
    "        fallback_dir = 'data/fordham-website'\n",
    "        if source_path != fallback_dir and os.path.exists(fallback_dir) and os.path.isdir(fallback_dir):\n",
    "             print(f\"Fallback: Loading from {fallback_dir}\")\n",
    "             return load_fordham_data(fallback_dir) # Recursive call to load from dir\n",
    "             \n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Usage\n",
    "source_path = 'data/fordham-website' # Pointing directly to the directory as confirmed\n",
    "if not os.path.exists(source_path):\n",
    "    print(f\"Warning: Path {source_path} does not exist relative to notebook.\")\n",
    "\n",
    "df = load_fordham_data(source_path)\n",
    "print(f\"Loaded {len(df)} documents.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Chunk the Documents\n",
    "\n",
    "Some of the pages could be too long to embed as a single unit. Down the line, the pages may be too long to stuff into the LLM's prompt during the generation step. As such, most of the RAG systems will break down big documents into into smaller **chunks**.\n",
    "\n",
    "> \ud83d\udcda **TERM: Chunking**  \n",
    "> Splitting documents into smaller, self-contained pieces for embedding and retrieval. The goal is chunks that are small enough to be specific, but large enough to be meaningful.\n",
    "\n",
    "Your task: **write a function that splits each document into chunks.**\n",
    "\n",
    "Things to think about:\n",
    "- What's a reasonable chunk size? (Think about what fits in a prompt vs. what's too vague)\n",
    "- Should you split on sentences? Paragraphs? A fixed character/word count?\n",
    "- Should chunks overlap? What happens if an answer spans two chunks?\n",
    "- How do you keep track of which document each chunk came from? You may need that information down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=100):\n",
    "    chunks = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "\n",
    "    start = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    while start < text_len:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "# Iterate over the loaded dataframe\n",
    "for index, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    url = row['url']\n",
    "    content = row['content']\n",
    "\n",
    "    chunks = chunk_text(content)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append({\n",
    "            'filename': filename,\n",
    "            'url': url,\n",
    "            'content': chunk\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(all_chunks)\n",
    "\n",
    "print(f\"Original documents: {len(df)}\")\n",
    "print(f\"Total chunks: {len(df_chunks)}\")\n",
    "print(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Embed the Chunks\n",
    "\n",
    "Now we need to turn each chunk into a vector so we can search over them. You've done this before in Lecture 4.\n",
    "\n",
    "Your task: **embed all chunks using an embedding model.**\n",
    "\n",
    "Tips:\n",
    "- You could use a local model, or API model. What are the tradeoffs?\n",
    "- This will take a while if you do it serially. You might want to use async/batch.\n",
    "- Once you've created your embeddings, you may want to save them to disk so you don't have to redo this step every time\n",
    "- You'll need to embed queries with the **same model** at search time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get the list of texts to embed\n",
    "chunk_texts = df_chunks['content'].tolist()\n",
    "\n",
    "print(f\"Embedding {len(chunk_texts)} chunks...\")\n",
    "\n",
    "# Embed the chunks\n",
    "# show_progress_bar=True is nice but might not show up well in non-interactive run, but good for notebook\n",
    "embeddings = model.encode(chunk_texts, show_progress_bar=True)\n",
    "\n",
    "# Add embeddings to the dataframe \n",
    "# We can store them as a list/array in a new column\n",
    "df_chunks['embedding'] = list(embeddings)\n",
    "\n",
    "print(\"Embedding complete.\")\n",
    "print(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Retrieve\n",
    "\n",
    "Now build the **R** in RAG. Given a user's question, find the most relevant chunks.\n",
    "\n",
    "Your task: **write a retrieval function that takes a question and returns the most relevant chunks.**\n",
    "\n",
    "Tips:\n",
    "- You can use lexical or semantic search or both!\n",
    "- How many chunks should you retrieve? Too few and you might miss the answer; too many and you'll overwhelm the LLM (and pay more tokens)\n",
    "- Try a few test questions and eyeball whether the retrieved chunks are relevant\n",
    "- Try a few questions and see what comes back. For example:\n",
    "  - \"What programs does the Gabelli School of Business offer?\"\n",
    "  - \"How do I apply for financial aid?\"\n",
    "  - \"Where is Fordham's campus?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve(query, df, top_k=5):\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode([query])[0]\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    # Stack the embeddings from the dataframe\n",
    "    embeddings = np.stack(df['embedding'].values)\n",
    "\n",
    "    # Normalize query and embeddings for cosine similarity\n",
    "    # (SentenceTransformer embeddings are usually normalized, but good practice)\n",
    "    query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "    embeddings_norm = embeddings / np.linalg.norm(embeddings, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Dot product\n",
    "    scores = np.dot(embeddings_norm, query_norm)\n",
    "\n",
    "    # valid_indices = np.argsort(scores)[::-1][:top_k] # This is slow for large arrays but fine here\n",
    "\n",
    "    # Get top k indices\n",
    "    # We can use argpartition for faster top-k\n",
    "    top_k_indices = np.argpartition(scores, -top_k)[-top_k:]\n",
    "    top_k_indices = top_k_indices[np.argsort(scores[top_k_indices])][::-1]\n",
    "\n",
    "    # Return the top k rows\n",
    "    return df.iloc[top_k_indices]\n",
    "\n",
    "# Test it\n",
    "results = retrieve(\"financial aid\", df_chunks)\n",
    "for i, row in results.iterrows():\n",
    "    print(f\"URL: {row['url']}\")\n",
    "    print(f\"Content: {row['content'][:150]}...\")\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Generate\n",
    "\n",
    "Now build the **G** in RAG. Take the retrieved chunks and pass them to an LLM along with the user's question.\n",
    "\n",
    "Your task: **write a function that takes a question and the retrieved chunks, builds a prompt, and calls an LLM to generate an answer.**\n",
    "\n",
    "Tips:\n",
    "- How should you structure the prompt? The LLM needs to know: (1) what is the context of the application, (2) what is the question, (3) what it should include in its answer\n",
    "- What should the LLM do if the context doesn't contain the answer?\n",
    "- Start with a cheap model; try a better one when you've figured out the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_fordham_data(zip_path):\n",
    "    data = []\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Zip file not found: {zip_path}\")\n",
    "        # Fallback to directory if zip is missing but directory exists?\n",
    "        # For now, just return empty or error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        # Get list of all files in the zip\n",
    "        file_list = [f for f in z.namelist() if f.endswith('.md')]\n",
    "\n",
    "        for file_name in file_list:\n",
    "            with z.open(file_name) as f:\n",
    "                # Read content and decode to string\n",
    "                try:\n",
    "                    content = f.read().decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "                # Split the first line (URL) from the rest of the text\n",
    "                lines = content.split('\\n', 1)\n",
    "                url = lines[0].strip() if lines else \"\"\n",
    "                body = lines[1].strip() if len(lines) > 1 else \"\"\n",
    "\n",
    "                data.append({\n",
    "                    \"filename\": file_name,\n",
    "                    \"url\": url,\n",
    "                    \"content\": body\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Usage\n",
    "zip_path = 'data/fordham-website.zip'\n",
    "df = load_fordham_data(zip_path)\n",
    "print(f\"Loaded {len(df)} documents.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Wire everything together\n",
    "\n",
    "Combine the previous steps into a simple function that takes in a question and returns an answer.\n",
    "\n",
    "Your task: **write a `rag(question)` function that retrieves relevant chunks and generates an answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_fordham_data(zip_path):\n",
    "    data = []\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Zip file not found: {zip_path}\")\n",
    "        # Fallback to directory if zip is missing but directory exists?\n",
    "        # For now, just return empty or error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        # Get list of all files in the zip\n",
    "        file_list = [f for f in z.namelist() if f.endswith('.md')]\n",
    "\n",
    "        for file_name in file_list:\n",
    "            with z.open(file_name) as f:\n",
    "                # Read content and decode to string\n",
    "                try:\n",
    "                    content = f.read().decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "                # Split the first line (URL) from the rest of the text\n",
    "                lines = content.split('\\n', 1)\n",
    "                url = lines[0].strip() if lines else \"\"\n",
    "                body = lines[1].strip() if len(lines) > 1 else \"\"\n",
    "\n",
    "                data.append({\n",
    "                    \"filename\": file_name,\n",
    "                    \"url\": url,\n",
    "                    \"content\": body\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Usage\n",
    "zip_path = 'data/fordham-website.zip'\n",
    "df = load_fordham_data(zip_path)\n",
    "print(f\"Loaded {len(df)} documents.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Evaluate, experiment and improve\n",
    "\n",
    "Your RAG system works \u2014 but there's always room to make it better. \n",
    "\n",
    "Your task: **evaluate, experiment, and improve your system**\n",
    "\n",
    "Tips:\n",
    "- How do you know that your system is working or that your changes are improving it?\n",
    "- Try different questions \u2014 where does it do well? Where does it struggle?\n",
    "- Adjust the number of retrieved chunks \u2014 what happens with more or fewer?\n",
    "- Try different chunking strategies \u2014 bigger chunks? Smaller? Overlap?\n",
    "- Try a different embedding model \u2014 does it change retrieval quality?\n",
    "- Improve the prompt \u2014 can you get better, more concise answers?\n",
    "- Add source attribution \u2014 can the system tell the user which pages the answer came from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_fordham_data(zip_path):\n",
    "    data = []\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"Zip file not found: {zip_path}\")\n",
    "        # Fallback to directory if zip is missing but directory exists?\n",
    "        # For now, just return empty or error\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        # Get list of all files in the zip\n",
    "        file_list = [f for f in z.namelist() if f.endswith('.md')]\n",
    "\n",
    "        for file_name in file_list:\n",
    "            with z.open(file_name) as f:\n",
    "                # Read content and decode to string\n",
    "                try:\n",
    "                    content = f.read().decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "\n",
    "                # Split the first line (URL) from the rest of the text\n",
    "                lines = content.split('\\n', 1)\n",
    "                url = lines[0].strip() if lines else \"\"\n",
    "                body = lines[1].strip() if len(lines) > 1 else \"\"\n",
    "\n",
    "                data.append({\n",
    "                    \"filename\": file_name,\n",
    "                    \"url\": url,\n",
    "                    \"content\": body\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Usage\n",
    "zip_path = 'data/fordham-website.zip'\n",
    "df = load_fordham_data(zip_path)\n",
    "print(f\"Loaded {len(df)} documents.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. (Optional) Make it an app\n",
    "\n",
    "So far your RAG system lives inside a notebook. That's great for development \u2014 but nobody is going to use your Jupyter notebook to ask questions about Fordham. Let's turn it into a real web app.\n",
    "\n",
    "> \ud83d\udcda **TERM: Streamlit**  \n",
    "> A Python library that turns plain Python scripts into interactive web apps. You write Python \u2014 no HTML, CSS, or JavaScript \u2014 and Streamlit renders it as a web page with inputs, buttons, and formatted output. It's the fastest way to go from \"I have a function\" to \"I have a web app.\"\n",
    "\n",
    "Your task: **create a Streamlit app that lets a user type a question about Fordham and get an answer from your RAG system.**\n",
    "\n",
    "To get started:\n",
    "- Install it: `uv pip install streamlit` \n",
    "- A Streamlit app is just a `.py` file (not a notebook). Create something like `fordham_rag_app.py`\n",
    "- Run it: `streamlit run scripts/fordham_rag_app.py` \u2014 this opens a browser tab with your app\n",
    "\n",
    "Tips:\n",
    "- Check out the [Streamlit docs](https://docs.streamlit.io/) \u2014 the \"Get started\" tutorial is very short\n",
    "- Your best bet is to vibecode your way to this. You'll be surprised how fast you can get it up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## What You Built\n",
    "\n",
    "| Step | What You Did | What It Does |\n",
    "|------|-------------|-------------|\n",
    "| **Load** | Read 9,500+ Fordham web pages | Get raw content |\n",
    "| **Chunk** | Split pages into smaller pieces | Make content searchable and promptable |\n",
    "| **Embed** | Turn chunks into vectors | Enable semantic search |\n",
    "| **Retrieve** | Find relevant chunks for a question | The **R** in RAG |\n",
    "| **Generate** | Ask an LLM to answer using the chunks | The **G** in RAG |\n",
    "| **RAG** | Wire it all together | Question in, answer out |\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "RAG is one of the most common patterns in AI engineering today. What you built here is the same core architecture behind tools like ChatGPT with search, Perplexity, enterprise Q&A bots, and more. The details get more sophisticated (vector databases, reranking, query rewriting, evaluation) but the pattern is the same:\n",
    "\n",
    "**Find relevant stuff \u2192 give it to an LLM \u2192 get an answer.**\n",
    "\n",
    "You can just build things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ce)",
   "language": "python",
   "name": "ce"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}